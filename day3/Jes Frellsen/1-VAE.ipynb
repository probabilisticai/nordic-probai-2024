{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Exercise 1: Variational autoencoders\n",
    "Exercise by [Jes Frellsen](https://frellsen.org) (Technical University of Denmark), June 2024 (version 1.0).\n",
    "\n",
    "In this programming exercise, you will work with variational autoencoders (VAEs). We consider a binarised version of the MNIST dataset, where pixels with values over $0.5$ are set to $1$ and pixels with values less than $0.5$ are set to $0$.\n",
    "\n",
    "The provided code is a modular and simple implementation of a VAE with\n",
    "* a Gaussian prior, $p(\\mathbf{z})$,\n",
    "* a product of Bernoulli likelihood, $p(\\mathbf{x}|\\mathbf{z})$,\n",
    "* a fully connected encoder and decoder network.\n",
    "\n",
    "The implementation makes use of [`torch.distributions`](https://pytorch.org/docs/stable/distributions.html) for the various distributions, which substantially simplifies the code.\n",
    "\n",
    "The code is followed by three exercises you have to solve.\n",
    "\n",
    "This implementation in the notebook takes inspiration from:\n",
    "* https://github.com/jmtomczak/intro_dgm/blob/main/vaes/vae_example.ipynb\n",
    "* https://github.com/kampta/pytorch-distributions/blob/master/gaussian_vae.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the VAE\n",
    "**VAE implementation:** Below we provide a complete implementation of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as td\n",
    "\n",
    "class GaussianPrior(nn.Module):\n",
    "    def __init__(self, M):\n",
    "        \"\"\"\n",
    "        Define a Gaussian prior distribution with zero mean and unit variance.\n",
    "\n",
    "                Parameters:\n",
    "        M: [int] \n",
    "           Dimension of the latent space.\n",
    "        \"\"\"\n",
    "        super(GaussianPrior, self).__init__()\n",
    "        self.M = M\n",
    "        self.mean = nn.Parameter(torch.zeros(self.M), requires_grad=False)\n",
    "        self.std = nn.Parameter(torch.ones(self.M), requires_grad=False)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Return the prior distribution.\n",
    "\n",
    "        Returns:\n",
    "        prior: [torch.distributions.Distribution]\n",
    "        \"\"\"\n",
    "        return td.Independent(td.Normal(loc=self.mean, scale=self.std), 1)\n",
    "\n",
    "\n",
    "class GaussianEncoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        \"\"\"\n",
    "        Define a Gaussian encoder distribution based on a given encoder network.\n",
    "\n",
    "        Parameters:\n",
    "        encoder_net: [torch.nn.Module]             \n",
    "           The encoder network that takes as a tensor of dim `(batch_size,\n",
    "           feature_dim1, feature_dim2)` and output a tensor of dimension\n",
    "           `(batch_size, 2M)`, where M is the dimension of the latent space.\n",
    "        \"\"\"\n",
    "        super(GaussianEncoder, self).__init__()\n",
    "        self.encoder_net = encoder_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Given a batch of data, return a Gaussian distribution over the latent space.\n",
    "\n",
    "        Parameters:\n",
    "        x: [torch.Tensor] \n",
    "           A tensor of dimension `(batch_size, feature_dim1, feature_dim2)`\n",
    "        \"\"\"\n",
    "        mean, std = torch.chunk(self.encoder_net(x), 2, dim=-1)\n",
    "        return td.Independent(td.Normal(loc=mean, scale=torch.exp(std)), 1)\n",
    "\n",
    "\n",
    "class BernoulliDecoder(nn.Module):\n",
    "    def __init__(self, decoder_net):\n",
    "        \"\"\"\n",
    "        Define a Bernoulli decoder distribution based on a given decoder network.\n",
    "\n",
    "        Parameters: \n",
    "        encoder_net: [torch.nn.Module]             \n",
    "           The decoder network that takes as a tensor of dim `(batch_size, M) as\n",
    "           input, where M is the dimension of the latent space, and outputs a\n",
    "           tensor of dimension (batch_size, feature_dim1, feature_dim2).\n",
    "        \"\"\"\n",
    "        super(BernoulliDecoder, self).__init__()\n",
    "        self.decoder_net = decoder_net\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Given a batch of latent variables, return a Bernoulli distribution over the data space.\n",
    "\n",
    "        Parameters:\n",
    "        z: [torch.Tensor] \n",
    "           A tensor of dimension `(batch_size, M)`, where M is the dimension of the latent space.\n",
    "        \"\"\"\n",
    "        logits = self.decoder_net(z)\n",
    "        return td.Independent(td.Bernoulli(logits=logits), 2)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a Variational Autoencoder (VAE) model.\n",
    "    \"\"\"\n",
    "    def __init__(self, prior, decoder, encoder):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        prior: [torch.nn.Module] \n",
    "           The prior distribution over the latent space.\n",
    "        decoder: [torch.nn.Module]\n",
    "              The decoder distribution over the data space.\n",
    "        encoder: [torch.nn.Module]\n",
    "                The encoder distribution over the latent space.\n",
    "        \"\"\"\n",
    "            \n",
    "        super(VAE, self).__init__()\n",
    "        self.prior = prior\n",
    "        self.decoder = decoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def elbo(self, x):\n",
    "        \"\"\"\n",
    "        Compute the ELBO for the given batch of data.\n",
    "\n",
    "        Parameters:\n",
    "        x: [torch.Tensor] \n",
    "           A tensor of dimension `(batch_size, feature_dim1, feature_dim2, ...)`\n",
    "           n_samples: [int]\n",
    "           Number of samples to use for the Monte Carlo estimate of the ELBO.\n",
    "        \"\"\"\n",
    "        q = self.encoder(x)\n",
    "        z = q.rsample()\n",
    "        elbo = torch.mean(self.decoder(z).log_prob(x) - td.kl_divergence(q, self.prior()), dim=0)\n",
    "        return elbo\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        \"\"\"\n",
    "        Sample from the model.\n",
    "        \n",
    "        Parameters:\n",
    "        n_samples: [int]\n",
    "           Number of samples to generate.\n",
    "        \"\"\"\n",
    "        z = self.prior().sample(torch.Size([n_samples]))\n",
    "        return self.decoder(z).sample()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the negative ELBO for the given batch of data.\n",
    "\n",
    "        Parameters:\n",
    "        x: [torch.Tensor] \n",
    "           A tensor of dimension `(batch_size, feature_dim1, feature_dim2)`\n",
    "        \"\"\"\n",
    "        return -self.elbo(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop**: We have also implemented a generic training loop for learning the VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, optimizer, data_loader, epochs, device):\n",
    "    \"\"\"\n",
    "    Train a Flow model.\n",
    "\n",
    "    Parameters:\n",
    "    model: [Flow]\n",
    "       The model to train.\n",
    "    optimizer: [torch.optim.Optimizer]\n",
    "         The optimizer to use for training.\n",
    "    data_loader: [torch.utils.data.DataLoader]\n",
    "            The data loader to use for training.\n",
    "    epochs: [int]\n",
    "        Number of epochs to train for.\n",
    "    device: [torch.device]\n",
    "        The device to use for training.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_steps = len(data_loader)*epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        data_iter = iter(data_loader)\n",
    "        for x in data_iter:\n",
    "            if isinstance(x, (list, tuple)):\n",
    "                x = x[0]\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=f\"â €{loss.item():12.4f}\", epoch=f\"{epoch+1}/{epochs}\")\n",
    "            progress_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training data**: Next, we load MNIST as binarized at `thresshold` and create data loaders for training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 32\n",
    "thresshold = 0.5\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (thresshold < x).float().squeeze())])\n",
    "\n",
    "mnist_train_loader = torch.utils.data.DataLoader(datasets.MNIST('data/', train=True, download=True, transform=transform), batch_size=batch_size, shuffle=True)\n",
    "mnist_test_loader = torch.utils.data.DataLoader(datasets.MNIST('data/', train=False, download=True, transform=transform), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the model and run the training loop**: Finally we initialize the model using a simple fully connected encoder and decoder networks and run the training loop. The networks are very simple to keep training time low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define prior distribution with dimension M\n",
    "M = 2\n",
    "prior = GaussianPrior(M)\n",
    "\n",
    "# Define encoder and decoder networks\n",
    "encoder_net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, M*2),\n",
    ")\n",
    "\n",
    "decoder_net = nn.Sequential(\n",
    "    nn.Linear(M, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 784),\n",
    "    nn.Unflatten(-1, (28, 28))\n",
    ")\n",
    "\n",
    "# Define VAE model\n",
    "decoder = BernoulliDecoder(decoder_net)\n",
    "encoder = GaussianEncoder(encoder_net)\n",
    "model = VAE(prior, decoder, encoder).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train model\n",
    "epochs = 10\n",
    "train(model, optimizer, mnist_train_loader, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling**: The following code samples from a trained model and plots the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    samples = (model.sample(64)).cpu()\n",
    "\n",
    "image_pil = to_pil_image(make_grid(samples.view(-1, 1, 28, 28)))\n",
    "display(image_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "In this first exercise, you should just inspect the VAE code above and answer the following questions:\n",
    "* How is the reparametrisation trick handled in the code?\n",
    "* Consider the implementation of the ELBO. What is the dimension of `self.decoder(z).log_prob(x)` and of `td.kl_divergence(q, self.prior.distribuion)`?\n",
    "* The implementation of the prior, encoder and decoder classes all make use of `td.Independent`. What does this do?\n",
    "* What is the purpose using the function `torch.chunk` in `GaussianEncoder.forward`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "Add the following functionally to the implementation of the VAE with Bernoulli output distributions:\n",
    "* Evaluate the ELBO on the binarised MNIST test set.\n",
    "* Plot samples from the approximate posterior and colour them by their correct class label for each datapoint in the test set (i.e., samples from the aggregate posterior) for a two-dimensional latent space, i.e., $M=2$.\n",
    "* *Optional:* The decoder and encoder networks above are very simple. Try to make the model more expressive by making the networks wider/deeper and increase the dimension of the latent space. Plot the aggregate posterior for $M>2$ by doing PCA and project the sample onto the first two principal components (e.g., using scikit-learn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "\n",
    "Extend the VAE with Bernoulli output distributions to use a mixture of Gaussian prior (MoG). We recommend using the [`MixtureSameFamily`](https://pytorch.org/docs/stable/distributions.html#mixturesamefamily) class from [`torch.distributions`](https://pytorch.org/docs/stable/distributions.html), but you are also welcome to implement it from scratch. For your implementation of the VAE with the MoG prior:\n",
    "* Evaluate the test set ELBO. Do you see better performance?\n",
    "* Plot the samples from the approximate posterior. How does it differ from the model with the Gaussian prior? Do you see better clustering?\n",
    "\n",
    "**Remark**: You will need to change the implementation of the ELBO."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_02456",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
