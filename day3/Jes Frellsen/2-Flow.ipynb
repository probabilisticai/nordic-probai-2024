{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Normalizing Flow\n",
    "Exercise by [Jes Frellsen](https://frellsen.org) (Technical University of Denmark), June 2024 (version 1.0).\n",
    "\n",
    "The main task in this programming exercise, is to implement the *masked coupling layer* from Real NVP ([Dinh et al., 2017](https://arxiv.org/abs/1605.08803)) on two simple 2D toy datasets. We have provided you with a file for the toy data:\n",
    "* `ToyData.py` contains the code for generating data from the two toy models.\n",
    "\n",
    "The code is followed by one exercise you have to solve.\n",
    "\n",
    "This implementation in the notebook takes inspiration from:\n",
    "* https://github.com/jmtomczak/intro_dgm/blob/main/flows/realnvp_example.ipynb\n",
    "* https://github.com/VincentStimper/normalizing-flows/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked coupling layer\n",
    "\n",
    "The advantage of using the masked coupling layer ([Dinh et al., 2017](https://arxiv.org/abs/1605.08803)) over a regular coupling layer, is that it allows for arbitrary partitioning of the variable in the coupling layer. Let $T: \\mathbb{R}^D \\to \\mathbb{R}^D$ be the masked coupling transformation that maps $\\mathbf{z}$ to $\\mathbf{z}'$. Then let $\\mathbf{b} \\in \\{0,1\\}^D$ denote a binary mask, where $b_i = 1$ means that $z_i$ is left unchanged by the coupling layer and $b_i = 0$ means that $z_i$ is transformed by the coupling layer. We can then express the masked coupling layer as the transformation\n",
    "$$\n",
    "    \\mathbf{z}' = \\mathbf{b} \\odot \\mathbf{z} + (\\mathbf{1}-\\mathbf{b}) \\odot \\left( \\mathbf{z} \\odot \\exp\\left(s(\\mathbf{b} \\odot \\mathbf{z})\\right) + t(\\mathbf{b} \\odot \\mathbf{z}) \\right)\n",
    "$$\n",
    "where $\\odot$ is the element wise product, $s:\\mathbb{R}^D \\to \\mathbb{R}^D$ is the network that calculates the scaling and $t:\\mathbb{R}^D \\to \\mathbb{R}^D$ is the network that calculates translation of the affine transformation. Note that the input to these networks are masked by multiplying $\\mathbf{z}$ by the mask, and that $\\mathbf{b} \\odot \\mathbf{z} = \\mathbf{b} \\odot \\mathbf{z}'$.\n",
    "\n",
    "The inverse of the transformation is given by an expression similar to the inverse of the regular couple layers, i.e.,\n",
    "$$\n",
    "    \\mathbf{z} = \\mathbf{b} \\odot \\mathbf{z}' + (\\mathbf{1}-\\mathbf{b}) \\odot\n",
    "    \\left( \\left(\\mathbf{z}' - t(\\mathbf{b} \\odot \\mathbf{z}')\\right)  \\odot \\exp\\left(-s(\\mathbf{b} \\odot \\mathbf{z}')\\right) \\right) ,\n",
    "$$\n",
    "and the log determinant of the Jacobian is given by\n",
    "\\begin{equation}\n",
    "    \\log \\left\\vert \\det \\mathbf{J}_T (z) \\right\\vert = \\sum_{d=1}^{D} (1-b_i) s_i(\\mathbf{b} \\odot \\mathbf{z}) .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download `ToyData.py` using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -O https://raw.githubusercontent.com/frellsen/ProbAI-2024/main/ToyData.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy data\n",
    "First we visualize the probability densities for the toy datasets.\n",
    "\n",
    "When we create an object of the `Chequerboard` or `TwoGaussian`, we can call the forward method which returns a `Distribution` object from `torch.distributions`. The `Distribution` class implements a method for calculating the log probability (`log_prob(...)`), which we will use to make the plots below, and a method for sampling from the distribution (`sample(...)`), which we will later use for creating our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ToyData\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Make a density plot of the Checkerboard distribution\n",
    "toy = ToyData.Chequerboard()\n",
    "coordinates = [[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)]\n",
    "prob = torch.exp(toy().log_prob(torch.tensor(coordinates)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "im = ax1.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax1.set_xlim(toy.xlim)\n",
    "ax1.set_ylim(toy.ylim)\n",
    "ax1.set_aspect('equal')\n",
    "cbar1 = fig.colorbar(im, ax=ax1)\n",
    "ax1.set_title('Checkerboard distribution')\n",
    "cbar1.set_label('Probability density')\n",
    "\n",
    "# Make a density plot of the Gaussian distribution\n",
    "toy = ToyData.TwoGaussians()\n",
    "coordinates = [[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)]\n",
    "prob = torch.exp(toy().log_prob(torch.tensor(coordinates)))\n",
    "\n",
    "im = ax2.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax2.set_xlim(toy.xlim)\n",
    "ax2.set_ylim(toy.ylim)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_title('Two Gaussians distribution')\n",
    "cbar2 = fig.colorbar(im, ax=ax2)\n",
    "cbar2.set_label('Probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Normalizing Flow\n",
    "**Flow implementation:** Below we provide an implementation of a flow (`Flow`) and a masked coupling layer (`MaskedCouplingLayer`). The masked coupling layer is missing the implementaion of the `forward`and `inverse` methods. Your task will be to complete them (see the exercise below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as td\n",
    "\n",
    "class GaussianBase(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        \"\"\"\n",
    "        Define a Gaussian base distribution with zero mean and unit variance.\n",
    "\n",
    "                Parameters:\n",
    "        M: [int] \n",
    "           Dimension of the base distribution.\n",
    "        \"\"\"\n",
    "        super(GaussianBase, self).__init__()\n",
    "        self.D = D\n",
    "        self.mean = nn.Parameter(torch.zeros(self.D), requires_grad=False)\n",
    "        self.std = nn.Parameter(torch.ones(self.D), requires_grad=False)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Return the base distribution.\n",
    "\n",
    "        Returns:\n",
    "        prior: [torch.distributions.Distribution]\n",
    "        \"\"\"\n",
    "        return td.Independent(td.Normal(loc=self.mean, scale=self.std), 1)\n",
    "\n",
    "class MaskedCouplingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An affine coupling layer for a normalizing flow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale_net, translation_net, mask):\n",
    "        \"\"\"\n",
    "        Define a coupling layer.\n",
    "\n",
    "        Parameters:\n",
    "        scale_net: [torch.nn.Module]\n",
    "            The scaling network that takes as input a tensor of dimension `(batch_size, feature_dim)` and outputs a tensor of dimension `(batch_size, feature_dim)`.\n",
    "        translation_net: [torch.nn.Module]\n",
    "            The translation network that takes as input a tensor of dimension `(batch_size, feature_dim)` and outputs a tensor of dimension `(batch_size, feature_dim)`.\n",
    "        mask: [torch.Tensor]\n",
    "            A binary mask of dimension `(feature_dim,)` that determines which features (where the mask is zero) are transformed by the scaling and translation networks.\n",
    "        \"\"\"\n",
    "        super(MaskedCouplingLayer, self).__init__()\n",
    "        self.scale_net = scale_net\n",
    "        self.translation_net = translation_net\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Transform a batch of data through the coupling layer (from the base to data).\n",
    "\n",
    "        Parameters:\n",
    "        x: [torch.Tensor]\n",
    "            The input to the transformation of dimension `(batch_size, feature_dim)`\n",
    "        Returns:\n",
    "        z: [torch.Tensor]\n",
    "            The output of the transformation of dimension `(batch_size, feature_dim)`\n",
    "        sum_log_det_J: [torch.Tensor]\n",
    "            The sum of the log determinants of the Jacobian matrices of the forward transformations of dimension `(batch_size, feature_dim)`.\n",
    "        \"\"\"\n",
    "        x = z\n",
    "        log_det_J = torch.zeros(z.shape[0])\n",
    "        return x, log_det_J\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        \"\"\"\n",
    "        Transform a batch of data through the coupling layer (from data to the base).\n",
    "\n",
    "        Parameters:\n",
    "        z: [torch.Tensor]\n",
    "            The input to the inverse transformation of dimension `(batch_size, feature_dim)`\n",
    "        Returns:\n",
    "        x: [torch.Tensor]\n",
    "            The output of the inverse transformation of dimension `(batch_size, feature_dim)`\n",
    "        sum_log_det_J: [torch.Tensor]\n",
    "            The sum of the log determinants of the Jacobian matrices of the inverse transformations.\n",
    "        \"\"\"\n",
    "        z = x\n",
    "        log_det_J = torch.zeros(x.shape[0])\n",
    "        return z, log_det_J\n",
    "\n",
    "\n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, base, transformations):\n",
    "        \"\"\"\n",
    "        Define a normalizing flow model.\n",
    "        \n",
    "        Parameters:\n",
    "        base: [torch.distributions.Distribution]\n",
    "            The base distribution.\n",
    "        transformations: [list of torch.nn.Module]\n",
    "            A list of transformations to apply to the base distribution.\n",
    "        \"\"\"\n",
    "        super(Flow, self).__init__()\n",
    "        self.base = base\n",
    "        self.transformations = nn.ModuleList(transformations)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Transform a batch of data through the flow (from the base to data).\n",
    "        \n",
    "        Parameters:\n",
    "        x: [torch.Tensor]\n",
    "            The input to the transformation of dimension `(batch_size, feature_dim)`\n",
    "        Returns:\n",
    "        z: [torch.Tensor]\n",
    "            The output of the transformation of dimension `(batch_size, feature_dim)`\n",
    "        sum_log_det_J: [torch.Tensor]\n",
    "            The sum of the log determinants of the Jacobian matrices of the forward transformations.            \n",
    "        \"\"\"\n",
    "        sum_log_det_J = 0\n",
    "        for T in self.transformations:\n",
    "            x, log_det_J = T(z)\n",
    "            sum_log_det_J += log_det_J\n",
    "            z = x\n",
    "        return x, sum_log_det_J\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        \"\"\"\n",
    "        Transform a batch of data through the flow (from data to the base).\n",
    "\n",
    "        Parameters:\n",
    "        x: [torch.Tensor]\n",
    "            The input to the inverse transformation of dimension `(batch_size, feature_dim)`\n",
    "        Returns:\n",
    "        z: [torch.Tensor]\n",
    "            The output of the inverse transformation of dimension `(batch_size, feature_dim)`\n",
    "        sum_log_det_J: [torch.Tensor]\n",
    "            The sum of the log determinants of the Jacobian matrices of the inverse transformations.\n",
    "        \"\"\"\n",
    "        sum_log_det_J = 0\n",
    "        for T in reversed(self.transformations):\n",
    "            z, log_det_J = T.inverse(x)\n",
    "            sum_log_det_J += log_det_J\n",
    "            x = z\n",
    "        return z, sum_log_det_J\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        \"\"\"\n",
    "        Compute the log probability of a batch of data under the flow.\n",
    "\n",
    "        Parameters:\n",
    "        x: [torch.Tensor]\n",
    "            The data of dimension `(batch_size, feature_dim)`\n",
    "        Returns:\n",
    "        log_prob: [torch.Tensor]\n",
    "            The log probability of the data under the flow.\n",
    "        \"\"\"\n",
    "        z, log_det_J = self.inverse(x)\n",
    "        return self.base().log_prob(z) + log_det_J\n",
    "    \n",
    "    def sample(self, sample_shape=(1,)):\n",
    "        \"\"\"\n",
    "        Sample from the flow.\n",
    "\n",
    "        Parameters:\n",
    "        n_samples: [int]\n",
    "            Number of samples to generate.\n",
    "        Returns:\n",
    "        z: [torch.Tensor]\n",
    "            The samples of dimension `(n_samples, feature_dim)`\n",
    "        \"\"\"\n",
    "        z = self.base().sample(sample_shape)\n",
    "        return self.forward(z)[0]\n",
    "    \n",
    "    def loss(self, x):\n",
    "        \"\"\"\n",
    "        Compute the negative mean log likelihood for the given data bath.\n",
    "\n",
    "        Parameters:\n",
    "        x: [torch.Tensor] \n",
    "            A tensor of dimension `(batch_size, feature_dim)`\n",
    "        Returns:\n",
    "        loss: [torch.Tensor]\n",
    "            The negative mean log likelihood for the given data batch.\n",
    "        \"\"\"\n",
    "        return -torch.mean(self.log_prob(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop**: We have also implemented a generic training loop for learning the DDPM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, optimizer, data_loader, epochs, device):\n",
    "    \"\"\"\n",
    "    Train a Flow model.\n",
    "\n",
    "    Parameters:\n",
    "    model: [Flow]\n",
    "       The model to train.\n",
    "    optimizer: [torch.optim.Optimizer]\n",
    "         The optimizer to use for training.\n",
    "    data_loader: [torch.utils.data.DataLoader]\n",
    "            The data loader to use for training.\n",
    "    epochs: [int]\n",
    "        Number of epochs to train for.\n",
    "    device: [torch.device]\n",
    "        The device to use for training.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_steps = len(data_loader)*epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        data_iter = iter(data_loader)\n",
    "        for x in data_iter:\n",
    "            if isinstance(x, (list, tuple)):\n",
    "                x = x[0]\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=f\"â €{loss.item():12.4f}\", epoch=f\"{epoch+1}/{epochs}\")\n",
    "            progress_bar.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training data**: Next, we generate some training data from the TwoGaussians datasets and create a `data_loader`. We generate a dataset with 10M data points and use a large batch size of 10,000. We can do so, since it is only a two-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "import ToyData\n",
    "\n",
    "batch_size = 10000\n",
    "n_data = 10000000\n",
    "\n",
    "toy = ToyData.TwoGaussians()\n",
    "train_loader = torch.utils.data.DataLoader(toy().sample((n_data,)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(toy().sample((n_data,)), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the model and run the training loop**: Finally we initialize the model using a simple fully connected encoder and decoder networks and run the training loop. *Remember that this will not work before you have completed the assignment below.*\n",
    "\n",
    "Here we define 10 transformations consisting of masked coupling layers, where we alternative between masking the first and second part of the input, and both the scale and translation nets have one hidden layer with 16 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Get the dimension of the dataset\n",
    "D = next(iter(train_loader)).shape[1]\n",
    "\n",
    "# Define prior distribution\n",
    "base = GaussianBase(D)\n",
    "\n",
    "# Define the transformations\n",
    "transformations =[]\n",
    "\n",
    "num_transformations = 5\n",
    "num_hidden = 8\n",
    "\n",
    "# Make a mask that is 1 for the first half of the features and 0 for the second half\n",
    "mask = torch.zeros((D,))\n",
    "mask[D//2:] = 1\n",
    "\n",
    "for i in range(num_transformations):\n",
    "    mask = (1-mask) # Flip the mask\n",
    "    scale_net = nn.Sequential(nn.Linear(D, num_hidden), nn.ReLU(), nn.Linear(num_hidden, D))\n",
    "    translation_net = nn.Sequential(nn.Linear(D, num_hidden), nn.ReLU(), nn.Linear(num_hidden, D))\n",
    "    transformations.append(MaskedCouplingLayer(scale_net, translation_net, mask))\n",
    "\n",
    "# Define flow model\n",
    "model = Flow(base, transformations).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train model\n",
    "epochs = 1\n",
    "train(model, optimizer, train_loader, epochs, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling**: The following code samples from a trained model and plots the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    samples = (model.sample((10000,))).cpu() \n",
    "\n",
    "# Plot the density of the toy data and the model samples\n",
    "coordinates = [[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)]\n",
    "prob = torch.exp(toy().log_prob(torch.tensor(coordinates)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "im = ax.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax.scatter(samples[:, 0], samples[:, 1], s=1, c='black', alpha=0.5)\n",
    "ax.set_xlim(toy.xlim)\n",
    "ax.set_ylim(toy.ylim)\n",
    "ax.set_aspect('equal')\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "In the provided code, the class `MaskedCouplingLayer` implements the masked coupling layer from Real NVP ([Dinh et al., 2017](https://arxiv.org/abs/1605.08803)), but it does not implement the forward transformation, the inverse transformation and the corresponding calculations of the log determinant of the Jacobian. \n",
    "    \n",
    "In this exercise you should complete the following two functions such that:\n",
    "* `MaskedCouplingLayer.forward(...)` returns $T(\\mathbf{z})$ and $\\log \\left\\vert\\det \\mathbf{J}_T (\\mathbf{z})\\right\\vert$.\n",
    "* `MaskedCouplingLayer.inverse(...)` returns $T^{-1}(\\mathbf{z}')$ and $\\log \\left\\vert\\det \\mathbf{J}_{T^{-1}} (\\mathbf{z}')\\right\\vert$.\n",
    "\n",
    "Use the TwoGaussians datasets for testing the model. Adjust the number of coupling layers and the architecture of the networks to get a good fit to the density (by qualitative assessment). Make sure to write your implementation such that it works on data of more than two dimensions.\n",
    "\n",
    "**Optional:** Can you also fit a flow to the Chequerboard dataset? It is difficult to find an architecture that gives a good fit.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_02456",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
