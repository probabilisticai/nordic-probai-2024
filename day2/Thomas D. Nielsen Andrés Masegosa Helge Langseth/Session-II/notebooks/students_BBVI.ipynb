{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PGM-Lab/2023-probai-private/blob/main/python/Day2-AfterLunch/notebooks/students_BBVI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10aXspkkJchq"
   },
   "source": [
    "# Set-up Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ShLegv-TAFz"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Standard imports. Not much to install this time\n",
    "%pip install -q numpy scipy matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "from dataclasses import dataclass\n",
    "from scipy.stats import multivariate_normal\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBT7J0rtJlvp"
   },
   "source": [
    "# Model and Data Generation\n",
    "\n",
    "Let's define a way to generate data from a linear regression model:\n",
    "\n",
    "$$ Y = 1.0 + 0.5 x + ϵ $$\n",
    "$$ ϵ \\sim N(0,0.1^2)$$\n",
    "\n",
    "The true parameters generating the data are thus $\\theta_0=1.0$ and $\\theta_1=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "QhZZKhF4TEhP",
    "outputId": "de0f4000-8cb7-4376-ceec-03f7d50e1ae8"
   },
   "outputs": [],
   "source": [
    "#@title \n",
    "def data_generator(offset: float, slope: float,\n",
    "                   observation_noise: float,\n",
    "                   no_samples: int = 10,\n",
    "                   x_range: tuple = None,) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Data generator, sampling from the model y_i = offset + slope * x_i + eps_i,\n",
    "    where eps_i is a Gaussian with mean 0 and a given noise-level.\n",
    "    If x_range is given, x-values are evenly spread out in that area.\n",
    "    If it is *not* given, we use [0, 1] as range\n",
    "    :param offset: Offset for the regression model\n",
    "    :param slope: Slope of the regression line\n",
    "    :param observation_noise: Standard-deviation of the noise-term\n",
    "    :param no_samples: Number of samples\n",
    "    :param x_range: Range for x-values. x_i-values will be evenly spaced within range\n",
    "    :return: a dict with two keys, 'x' and 'y'. Each corresponding value is a numpy array\n",
    "    \"\"\"\n",
    "    x_range = (0, 1) if x_range is None else x_range\n",
    "    x = np.linspace(start=x_range[0], stop=x_range[1], num=no_samples)\n",
    "    y = offset + slope * x + np.random.randn(no_samples) * observation_noise\n",
    "    return {'x': x, 'y': y}\n",
    "\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(12345)\n",
    "no_observations = 5\n",
    "data_noise = .1  # Model is y_i = w0 + w1*x + Normal(0, data_noise^2)\n",
    "sample = data_generator(offset=1., slope=0.5, no_samples=no_observations, observation_noise=data_noise)\n",
    "\n",
    "plt.title(\"Training Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.scatter(sample['x'], sample['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k147nsaRLE9n"
   },
   "source": [
    "# Computing $\\ln p(D,\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BT35Sih3YrFB"
   },
   "source": [
    "Next, code for computing $\\ln p(D,\\theta)$ for a linear regression model:\n",
    "$$\\theta_j \\sim N(0,10)  \\text{ a priori for }j=1, 2$$\n",
    "$$p(y|x,\\theta_0,\\theta_1) = N(\\theta_0 x + \\theta_1, 0.1^2)$$\n",
    "\n",
    "**Notice** here that the parameters for $p(\\theta)$ is defined using a specific *data-class* called `Parameters`. This means we get two slots of information: `prior.mean` and `prior.std`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EH2lNyTcTCtg"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# We define a \"dataclass\" to store distributional parameters for a Gaussian. \n",
    "# The class expects one vector for the mean, andother for the standard deviation. \n",
    "# We will use it for 2D distributions, but hard-code that the two components are \n",
    "# independent (as the standard deviations only define the diagonal of the covariance matrix). \n",
    "\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    mean: np.ndarray    # Mean of some 2-variable Gaussian distribution. A numpy-vector of size (2,)\n",
    "    std: np.ndarray     # Standard deviations of some 2-variable Gaussian distribution. A numpy-vector of size (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "SpfLYajzTF6z"
   },
   "outputs": [],
   "source": [
    "#@title Define the log-probability using the data structure\n",
    "def log_p(data: dict,\n",
    "          theta: np.ndarray,\n",
    "          prior: Parameters,\n",
    "          observation_noise: float) -> Union[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate log p(data , theta)\n",
    "    :param observation_noise: Noise in regression model\n",
    "    :param data: Dict with data. Keys are 'x' and 'y'\n",
    "    :param theta: Values for theta: theta[0] is offset, theta[1] is slope\n",
    "    :param prior: Parameters describing the prior distribution\n",
    "    :return: log probability of data and theta\n",
    "    \"\"\"\n",
    "\n",
    "    # Here we do some extras to make sure that we can take in many thetas at the time (needed for contour plots)\n",
    "    if len(theta.shape) == 1:\n",
    "        theta = np.reshape(theta, (1, -1))\n",
    "\n",
    "    # Log prior, i.e., log p(theta) under the prior distribution. Outsource this to scipy's multivariate_normal\n",
    "    log_prior_probs = multivariate_normal(mean=prior.mean, cov=prior.std**2).logpdf(theta)\n",
    "\n",
    "    # Some extra efforts here for vectorization of the code needed for the plotting procedure to be efficient\n",
    "    locs = np.reshape(theta[:, 0], (-1, 1)) + np.reshape(data['x'], (1, -1)) * np.reshape(theta[:, 1], (-1, 1))\n",
    "    log_likelihoods = np.sum(norm(loc=locs, scale=observation_noise).logpdf(\n",
    "        np.reshape(data['y'], (1, -1))), axis=-1)\n",
    "\n",
    "    # We need the sum: log p(theta) + log p( data | theta)\n",
    "    return_value = log_prior_probs + log_likelihoods\n",
    "\n",
    "    # Squeeze if possible -- again due to the vectorization of the code\n",
    "    if return_value.size == 1:\n",
    "        return_value = float(return_value[0])\n",
    "\n",
    "    # We are done.\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrTSo0mVMNtp"
   },
   "source": [
    "# Computing  $\\ln q(\\theta|\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n109sElmuVL"
   },
   "source": [
    "In our case, we have **two parameters** $\\theta=(\\theta_0,\\theta_1)$ and the **variational family** $q(\\theta|\\lambda)$ can be expressed as:\n",
    "\n",
    "$$q(\\theta|\\lambda) = q_0(\\theta_0|\\mu_0,\\sigma_0)q_1(\\theta_1|\\mu_1,\\sigma_1)$$\n",
    "\n",
    "$$q_i(\\theta_i|\\mu_i,\\sigma_i) = N(\\theta_i|\\mu_i,\\sigma_i)\\quad i=0,1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLA2v5LIMM5t"
   },
   "outputs": [],
   "source": [
    "def log_q(theta: np.ndarray,\n",
    "          q_distribution: Parameters,\n",
    "          ) -> Union[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate log q(theta | lambda)\n",
    "    :param theta: Parameters for theta: theta[0] is offset, theta[1] is slope in our regression model\n",
    "    :param q_distribution: Lambda-parameters. As for theta, we use position[0] is for offset, \n",
    "    and position [1] is for slope, so q_distribution.mean[0] is the expected value E_q[theta_0], etc.\n",
    "    :return: Log variational probability of theta under q(theta|lambda)\n",
    "    \"\"\"\n",
    "\n",
    "    # Here we do some extras to make sure that we can take in many thetas at the time (needed for contour plots)\n",
    "    if len(theta.shape) == 1:\n",
    "        theta = np.reshape(theta, (1, -1))\n",
    "\n",
    "    return_value = multivariate_normal(mean=q_distribution.mean, cov=q_distribution.std**2).logpdf(theta)\n",
    "\n",
    "    # Squeeze if possible\n",
    "    if isinstance(return_value, np.ndarray) and return_value.size == 1:\n",
    "        return_value = float(return_value[0])\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW_APv_MMTPg"
   },
   "source": [
    "# Computing the ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G7tLhXYmuVM"
   },
   "source": [
    "Calculation of the ELBO for linear regression. This implementation is not altered by us doing gradient-based ELBO. It is included simply for plotting and monitoring convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4q5u2p-TL7p"
   },
   "outputs": [],
   "source": [
    "def calculate_evidence_lower_bound(\n",
    "        data: Dict[str, np.ndarray],\n",
    "        q_distribution: Parameters,\n",
    "        prior: Parameters,\n",
    "        observation_noise: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper routine: Calculate ELBO.\n",
    "    Note: This function obviously only works for this particular model and is not a general solution.\n",
    "\n",
    "    :param data: Data is the sampled x and y values in a dictionary\n",
    "    :param q_distribution: Representation for the variational distribution\n",
    "    :param prior: Prior distribution p(theta)\n",
    "    :param observation_noise:  The data noise for y\n",
    "    :return: The calculated ELBO\n",
    "    \"\"\"\n",
    "\n",
    "    # We calculate the ELBO as E_q log p(data, theta) - E_q log q(theta), where\n",
    "    # log p(data, theta) = sum_i log p(y_i|x_i,theta) + log p(theta_0) + log p(theta_1)\n",
    "    # and \n",
    "    # log q(theta) = log q(theta_0) + log q(theta_2)\n",
    "\n",
    "    # \n",
    "    # E_q log p(theta_j) = -0.5 * log(2 * np.pi) - log(prior sigma) - E_q [theta_j**2] / (2*prior_sigma^2)\n",
    "    # First for theta[0]\n",
    "    expected_log_p = \\\n",
    "        - .5 * np.log(2 * np.pi) \\\n",
    "        - .5 * np.log(prior.std[0])  \\\n",
    "        - .5 * (q_distribution.mean[0] ** 2 + q_distribution.std[0] ** 2) / (prior.std[0] ** 2)\n",
    "    # Add contribution from theta[1]\n",
    "    expected_log_p += \\\n",
    "        - .5 * np.log(2 * np.pi) \\\n",
    "        - .5 * np.log(prior.std[1]) \\\n",
    "        - .5 * (q_distribution.mean[1] ** 2 + q_distribution.std[1] ** 2) / (prior.std[1] ** 2)\n",
    "\n",
    "    # E_q log p(y_i|x,theta)\n",
    "    expected_theta_squared = q_distribution.mean ** 2 + q_distribution.std ** 2\n",
    "    for i in range(data['x'].shape[0]):\n",
    "        x_i, y_i = data['x'][i], data['y'][i]\n",
    "        expected_log_p += -0.5 * np.log(2 * np.pi) - 0.5 * np.log(observation_noise) \\\n",
    "                          - 0.5/(observation_noise**2) * (\n",
    "                                  y_i ** 2\n",
    "                                  - 2 * y_i * q_distribution.mean[0]\n",
    "                                  - 2 * y_i * x_i * q_distribution.mean[1]\n",
    "                                  + expected_theta_squared[0]\n",
    "                                  + x_i ** 2 * expected_theta_squared[1]\n",
    "                                  + 2 * x_i * q_distribution.mean[0] * q_distribution.mean[1]\n",
    "                          )\n",
    "    # Entropy of q\n",
    "    ent = multivariate_normal(mean=q_distribution.mean, cov=q_distribution.std**2).entropy()\n",
    "    return expected_log_p + ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzfEIwI8M33B"
   },
   "source": [
    "# Auxiliary Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekSLs3ItmuVN"
   },
   "source": [
    "The `GradientInfo` dataclass is used to store gradient information. The main slot is `gradient`, which will hold the gradient value. Since we have gradient for all parameters in $\\boldsymbol\\lambda$, we again use a `Parameters` dataclass to keep hold of the numerical quantities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJLHcLQKLkIQ"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "@dataclass\n",
    "class GradientInfo:\n",
    "    theta: np.ndarray\n",
    "    score_function: Optional[Parameters]\n",
    "    gradient: Parameters\n",
    "    aggregation: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDpykGTTmuVN"
   },
   "source": [
    "We use `estimate_gradient_by_sampling` to generate the gradient estimate: Run through generating a number of samples, and average it. A bit of extras for making nice plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hI9LMyzFTRpL"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def estimate_gradient_by_sampling(\n",
    "        data: Dict[str, np.ndarray],\n",
    "        q_distribution: Parameters,\n",
    "        prior: Parameters,\n",
    "        observation_noise: float,\n",
    "        gradient_estimator: callable,\n",
    "        no_samples: int = 1\n",
    ") -> Tuple[Parameters, List[GradientInfo]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Run the gradient estimation. A number of gradient estimates are found, and averaged. \n",
    "    \n",
    "    :param data: The data. A dict with two keys: 'x' and 'y'\n",
    "    :param q_distribution:  The parameters ofbthe q-distribution\n",
    "    :param prior: The parameters of the prior\n",
    "    :param observation_noise: The observation noise in the likelihood model \n",
    "    :param gradient_estimator: The callable used to generate a sample-point with associated gradient\n",
    "    :param no_samples: The number of samples to make and average over in the estimate. Called $M$ in the slides\n",
    "    :return: First a Parameter-object with the gradient, then a list of GradientInfo-s. \n",
    "        The latter is only used for plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # aggregated: Data storage that keeps track of the sum of individual gradient as we move on\n",
    "    aggregated = Parameters(mean=np.array([0., 0.]), std=np.zeros((2, )))\n",
    "    \n",
    "    # parts: a list to keep track of the gradient-information from each of the samples. \n",
    "    parts = []\n",
    "\n",
    "    for _ in range(no_samples):\n",
    "\n",
    "        # Find gradient at a random location. What this means is outsorced to the \n",
    "        # gradient_estimator callable\n",
    "        sampled_gradient = gradient_estimator(\n",
    "            data=data,\n",
    "            q_distribution=q_distribution,\n",
    "            observation_noise=observation_noise,\n",
    "            prior=prior,\n",
    "        )\n",
    "        # Add up\n",
    "        aggregated.mean += sampled_gradient.gradient.mean\n",
    "        aggregated.std += sampled_gradient.gradient.std\n",
    "        parts += [sampled_gradient]\n",
    "\n",
    "    # Aggregated is average move, not sum of moves as so far\n",
    "    aggregated.mean /= no_samples\n",
    "    aggregated.std /= no_samples\n",
    "    \n",
    "    # Remember the gradient information, too\n",
    "    parts += [\n",
    "        GradientInfo(\n",
    "            gradient=aggregated,\n",
    "            aggregation=True,\n",
    "            score_function=None,\n",
    "            theta=q_distribution.mean)\n",
    "    ]\n",
    "    \n",
    "    # Done\n",
    "    return aggregated, parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5-IHE9JmuVO"
   },
   "source": [
    "The next function is only for plotting contour plots and gradient estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2Z_J1Mp3TJXR"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "def plot_pdfs(q_distribution: Parameters,\n",
    "              prior: Parameters,\n",
    "              observation_noise: float,\n",
    "              data: Dict[str, np.ndarray],\n",
    "              title: str = None,\n",
    "              arrows: List[GradientInfo] = None,\n",
    "              filename: str = None) -> None:\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find some range for parameters, and make ranges for w0, w1\n",
    "\n",
    "    # Fairly zoomed in\n",
    "    w0_list = np.linspace(0., 2., 100)\n",
    "    w1_list = np.linspace(0., 1., 100)\n",
    "\n",
    "    # Bird's eye view\n",
    "    # w0_list = np.linspace(-3, 6., 100)\n",
    "    # w1_list = np.linspace(-1., 3., 100)\n",
    "\n",
    "    w0_mesh, w1_mesh = np.meshgrid(w0_list, w1_list)\n",
    "\n",
    "    plt.Figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    for arrow in arrows:\n",
    "        plt.arrow(\n",
    "            x=arrow.theta[0],\n",
    "            y=arrow.theta[1],\n",
    "            dx=arrow.gradient.mean[0] * 1E-3,\n",
    "            dy=arrow.gradient.mean[1] * 1E-3,\n",
    "            length_includes_head=True,\n",
    "            color='g' if arrow.aggregation else 'k'\n",
    "        )\n",
    "        if arrow.score_function is not None:\n",
    "            plt.arrow(\n",
    "                x=arrow.theta[0],\n",
    "                y=arrow.theta[1],\n",
    "                dx=arrow.score_function.mean[0] * 1E-3,\n",
    "                dy=arrow.score_function.mean[1] * 1E-3,\n",
    "                length_includes_head=True,\n",
    "                color=\"r\"\n",
    "            )\n",
    "\n",
    "        plt.plot(\n",
    "            arrow.theta[0],\n",
    "            arrow.theta[1],\n",
    "            f\"{'g' if arrow.aggregation else 'k'}o\",\n",
    "            markersize=3.\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Plot p(theta|Data). Note, since we use contours here, and p(Data) is constant in theta, we\n",
    "    # plot p(theta, data) instead -- won't make a difference and easier since it is already available\n",
    "    theta_vector = np.vstack([w0_mesh.flatten(), w1_mesh.flatten()]).T\n",
    "    ax.contour(\n",
    "        w0_mesh,\n",
    "        w1_mesh,\n",
    "        np.exp(log_p(\n",
    "            data=data,\n",
    "            theta=theta_vector,\n",
    "            prior=prior,\n",
    "            observation_noise=observation_noise\n",
    "        )).reshape(w0_mesh.shape),\n",
    "        colors='b',\n",
    "        linewidths=.5,\n",
    "        linestyles='dashed',\n",
    "        levels=5,\n",
    "        alpha=1.\n",
    "    )\n",
    "\n",
    "    # Plot log-q\n",
    "    ax.contour(\n",
    "        w0_mesh,\n",
    "        w1_mesh,\n",
    "        np.exp(log_q(\n",
    "            q_distribution=q_distribution,\n",
    "            theta=theta_vector,\n",
    "        )).reshape(w0_mesh.shape),\n",
    "        colors='g',\n",
    "        linewidths=1.,\n",
    "        linestyles='dashed',\n",
    "        levels=5,\n",
    "        alpha=1.\n",
    "    )\n",
    "\n",
    "    # Fix the layout\n",
    "    plt.xlim([w0_list[0], w0_list[-1]])\n",
    "    plt.ylim([w1_list[0], w1_list[-1]])\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel('$\\\\theta_0$')\n",
    "    plt.ylabel('$\\\\theta_1$')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQsC2qCEmuVO"
   },
   "source": [
    "We use `compute_gradients` to run the show: Initiallize $q$ at a suitable location, then do the learning iteratively. It will update the results it moves along, and even dump some plots every now and then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XOLQ65jGNZNQ"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "store_elbo = {}\n",
    "\n",
    "def compute_gradients(gradient_method: dict, \n",
    "                      no_samples_per_iter: int = 5, \n",
    "                      learning_rate: float = 0.0001, \n",
    "                      no_iter: int = 1000) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    The main entry point for testing a gradient-based method. Implements a \n",
    "        naive (fixed-learning-rate) optimizer.\n",
    "    \n",
    "    :param gradient_method: A dictionary containing two keys: \n",
    "        'name' that gives the name of the method, and \n",
    "        'callable', that gives the callable that calculates gradients\n",
    "    \n",
    "    :param no_samples_per_iter: The number of samples used to approximate \n",
    "        the expectation by sampling. Called M in the slides\n",
    "    :param learning_rate: The learning rate. \n",
    "    :param no_iter: The number of iterations\n",
    "    \"\"\"\n",
    "\n",
    "    # Define our weight prior\n",
    "    prior_weight_sigma = 10.\n",
    "    prior_distribution = Parameters(\n",
    "        mean=np.array([0, 0]),\n",
    "        std=np.array([prior_weight_sigma, prior_weight_sigma])\n",
    "    )\n",
    "\n",
    "    # Define starting-point for the variational iterations\n",
    "    variational_distribution = Parameters(\n",
    "        mean=np.array([0., 0.]),\n",
    "        std=np.array([prior_weight_sigma, prior_weight_sigma])\n",
    "    )\n",
    "\n",
    "    # Set aside some storage for ELBOs obtained by the method\n",
    "    store_elbo[gradient_method['name']] = np.empty((no_iter,))\n",
    "\n",
    "    for iteration in range(no_iter):\n",
    "        # Calculate the score gradient.\n",
    "        # The gradient-function returns to things: The actual gradient (a Parameters-typed object),\n",
    "        # and a list of GradientInfo objects. The latter is for plotting only\n",
    "        grad, plot_arrows = estimate_gradient_by_sampling(\n",
    "            gradient_estimator=gradient_method['callable'],\n",
    "            data=sample,\n",
    "            q_distribution=variational_distribution,\n",
    "            prior=prior_distribution,\n",
    "            observation_noise=data_noise,\n",
    "            no_samples=no_samples_per_iter\n",
    "        )\n",
    "\n",
    "        # Plotting\n",
    "        if (iteration + 1) % 50 == 0:\n",
    "            plot_pdfs(\n",
    "                q_distribution=variational_distribution,\n",
    "                data=sample,\n",
    "                prior=prior_distribution,\n",
    "                observation_noise=data_noise,\n",
    "                arrows=plot_arrows,\n",
    "                title=f\"{gradient_method['name']} -- Iteration {iteration + 1}\",\n",
    "                filename=f\"iteration{iteration + 1}_{gradient_method['name']}.pdf\"\n",
    "            )\n",
    "        variational_distribution.mean += grad.mean * learning_rate\n",
    "        variational_distribution.std += grad.std * learning_rate\n",
    "\n",
    "        # Take some care: With very noisy gradients (small M), too large learning rate,\n",
    "        # and no fancy re-parameterization of the standard deviation in q, this vanilla\n",
    "        # version can run into settings where the estimated standard deviations get negative.\n",
    "        # If this happens we simply undo the move, and hope for a better sample the next time...\n",
    "        if np.min(variational_distribution.std) < 1E-3:\n",
    "            variational_distribution.mean -= grad.mean * learning_rate\n",
    "            variational_distribution.std -= grad.std * learning_rate\n",
    "            warnings.warn(\n",
    "                f\"\\nHad to undo move in iteration {iteration + 1}:\\n\"\n",
    "                f\"\\t- Learning rate: {learning_rate}\\n\"\n",
    "                f\"\\t- M = {no_samples_per_iter}\\n\"\n",
    "                f\"\\t- Gradient lengths {np.linalg.norm(grad.mean):.2f} (mean) and \"\n",
    "                f\"{np.linalg.norm(grad.std):.2f} (std).\\n\"\n",
    "                f\"\\t- Current lambda: {variational_distribution.mean} (mean) and \"\n",
    "                f\"{variational_distribution.std} (std).\\n\"\n",
    "                f\"Anyway. No harm done -- will just keep on going!\"\n",
    "            )\n",
    "\n",
    "        # Calculate ELBO and store it for later plotting\n",
    "        store_elbo[gradient_method['name']][iteration] = \\\n",
    "            calculate_evidence_lower_bound(\n",
    "                data=sample,\n",
    "                q_distribution=variational_distribution,\n",
    "                observation_noise=data_noise,\n",
    "                prior=prior_distribution,\n",
    "            )\n",
    "        \n",
    "        # Dump some info\n",
    "        if (iteration + 1) % 50 == 0:\n",
    "          print(f\"{gradient_method['name']} -- Iteration {iteration + 1:6d} / {no_iter}, M = {no_samples_per_iter}: \"\n",
    "                f\"q is N({variational_distribution.mean[0]:6.3f}, {variational_distribution.std[0]:6.3f}) and \"\n",
    "                f\"N({variational_distribution.mean[1]:6.3f}, {variational_distribution.std[1]:6.3f}) -> \"\n",
    "                f\"ELBO = {store_elbo[gradient_method['name']][iteration]:15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7HdZXxSMawC"
   },
   "source": [
    "# Exercise 1. Score Function Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ODCf3EyZ3oe"
   },
   "source": [
    "* In this first exercise, we ask you to implement a key part of the score function method to compute gradients. As we showed in Slide 6, the score function method computes the gradient of the ELBO in the following way:\n",
    "\n",
    "$$\\nabla_\\lambda {\\cal L} (\\lambda)= \\frac{1}{M} \\sum_{j=1}^M \\ln \\frac{p(D,\\theta^{(j)})}{q(\\theta^{(j)}|\\lambda)}∇_λ \\ln q(\\theta^{(j)}|\\lambda)$$\n",
    "where $\\theta^{(j)} \\sim q(\\theta|\\lambda)$. \n",
    "\n",
    "* Remember, we have a **linear regression model**\n",
    "$$\\theta_0, \\theta_1 \\sim N(0,10)$$\n",
    "$$y|x,\\theta_0,\\theta_1 \\sim N(\\theta_0 x + \\theta_1, 0.1)$$\n",
    "\n",
    "* In our case, we have **two parameters** $\\theta=(\\theta_0,\\theta_1)$ and the **variational family** $q(\\theta|\\lambda)$ can be expressed as:\n",
    "\n",
    "$$q(\\theta|\\lambda) = q_0(\\theta_0|\\mu_0,\\sigma_0)q_1(\\theta_1|\\mu_1,\\sigma_1)$$\n",
    "\n",
    "$$q_i(\\theta_i|\\mu_i,\\sigma_i) = N(\\theta_i|\\mu_i,\\sigma_i)\\quad i=0,1$$\n",
    "\n",
    "\n",
    "For your reference, \n",
    "$$\\log q_i(\\theta_i|\\mu_i, \\sigma_i) = -1/2\\log(2\\pi)-\\log\\sigma - \\frac{1}{2\\sigma^2}(θ_i-\\mu_i)^2$$\n",
    "\n",
    "\n",
    "\n",
    "* **For a given sample $\\theta$ from $q(\\theta|\\lambda)$**, we have to:\n",
    "\n",
    "\n",
    "\n",
    "> 1. **Compute its associated weight** (this is coded below): \n",
    "$$\\ln \\frac{p(D,\\theta)}{q(\\theta|\\lambda)} = \\ln p(D,\\theta) - \\ln q(\\theta|\\lambda)$$ \n",
    "\n",
    "> 2. **Compute the gradient** of $\\ln q(\\theta|\\lambda)$, i.e., $∇_λ \\ln q(\\theta|\\lambda$). \n",
    "\n",
    "We now ask you to derive and these gradient for the linear regression model. This requires both $∇_{\\mu_i}\\ln q_i(\\theta_i|\\mu_i,\\sigma_i)$ and $∇_{\\sigma_i}\\ln q_i(\\theta_i|\\mu_i,\\sigma_i)$, but to help you get going we give the result wrt. $\\sigma$:\n",
    "\n",
    "$$∇_{\\sigma_i}\\ln q_i(\\theta_i|\\mu_i,\\sigma_i) = \\frac{(\\theta_i-\\mu_i)^2}{\\sigma_i^3} - \\frac{1}{\\sigma_i}\n",
    "= \\left(\\left(\\frac{\\theta_i-\\mu_i}{\\sigma_i}\\right)^2 - 1\\right)\\cdot \\frac{1}{\\sigma_i}\n",
    "\\quad i=0,1$$\n",
    "\n",
    "$$∇_{\\mu_i}\\ln q_i(\\theta_i|\\mu_i,\\sigma_i) = \\color{red}\\textbf{????}\\color{black} \\quad i=0,1$$\n",
    "\n",
    "\n",
    "**TASK FOR THIS EXERCISE:** Find the gradient with respect to $\\mu$, and implement it. Test the solution in the cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2gGg1jqTQLn"
   },
   "outputs": [],
   "source": [
    "def score_function_gradient(\n",
    "        q_distribution: Parameters,\n",
    "        prior: Parameters,\n",
    "        observation_noise: float,\n",
    "        data: Dict[str, np.ndarray],\n",
    ") -> GradientInfo:\n",
    "\n",
    "    \"\"\"\n",
    "    Sample a theta, and find gradient at that theta defined as\n",
    "         (log p(d, theta) - log q(theta|lambda)) * gradient_lambda log q(theta|lambda)\n",
    "         \n",
    "    :param q_distribution: Parameters for the q-distribution\n",
    "    :param prior: Parameters for the prior distribution\n",
    "    :param observation_noise: The observation noise in the regression model\n",
    "    :param data: A dictionary with the data. Has two keys: 'x' and 'y'\n",
    "    :return: The gradient info. \n",
    "    \n",
    "    \"\"\"\n",
    "    # Sample a location\n",
    "    theta_sample = multivariate_normal(mean=q_distribution.mean, cov=q_distribution.std ** 2).rvs(size=1)\n",
    "\n",
    "    # Weight is log p(d, theta) - log q(theta|lambda). We have functions to calculate each part\n",
    "    weight = log_p(data=data, theta=theta_sample, prior=prior, observation_noise=observation_noise) - \\\n",
    "             log_q(theta=theta_sample, q_distribution=q_distribution)\n",
    "\n",
    "\n",
    "    # for sigma the score function is  1/sigma * [(theta - mu)**2/sigma**2 - 1] -- holds for each dimension\n",
    "    score_function = Parameters(\n",
    "        #\n",
    "        # THIS IS WHERE YOU PUT YOUR SOLUTION. \n",
    "        # Calculate the gradient of log q wrt \\mu.\n",
    "        # The (0, 0) allocation we have put in as a placeholder should be overwritten! \n",
    "        #\n",
    "        mean = np.zeros((2, )),\n",
    "\n",
    "        # The gradient for \\sigma is calculated already, and implemented here\n",
    "        std=np.array([\n",
    "            (((theta_sample[0] - q_distribution.mean[0]) / q_distribution.std[0])**2 - 1) / q_distribution.std[0],\n",
    "            (((theta_sample[1] - q_distribution.mean[1]) / q_distribution.std[1]) ** 2 - 1) / q_distribution.std[1]\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Gradients are score-function times the weights calculated above\n",
    "    gradient = Parameters(\n",
    "        mean=weight * score_function.mean,\n",
    "        std=weight * score_function.std\n",
    "    )\n",
    "\n",
    "    # Package everything so that we have plotting information as well as the answer\n",
    "    solution = GradientInfo(\n",
    "        theta=theta_sample,\n",
    "        score_function=score_function,\n",
    "        gradient=gradient,\n",
    "        aggregation=False\n",
    "    )\n",
    "\n",
    "    # Done!\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiNORn1EmuVQ"
   },
   "source": [
    "As soon as the gradients are calculated correctly we can package a dict to contain information about it and run `compute_gradients`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNgV8baGPBS_"
   },
   "outputs": [],
   "source": [
    "score_function = {'name': 'Score-function', 'callable': score_function_gradient}\n",
    "compute_gradients(score_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy7hXaTiPArL"
   },
   "source": [
    "# Exercise 2. Reparametrization Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddmxOitjkMNt"
   },
   "source": [
    "* In this second exercise, we ask you to implement a key part of the reparametrization trick method to compute the gradients of the ELBO. As we showed in Slide 11, the reparametrization trick method computes the gradient of the ELBO in the following way:\n",
    "\n",
    "$$\\nabla_\\lambda {\\cal L} (\\lambda) = \\frac{1}{M} \\sum_{j=1}^M ∇_\\theta \\ln \\frac{p(D,\\theta^{(j)})}{q(\\theta^{(j)}|\\lambda)}∇_λ f(ϵ^{(j)},\\lambda)$$\n",
    "where $\\epsilon^{(j)} \\sim N(0|I)$ and  $\\theta^{(j)}=f(\\epsilon^{(j)},\\lambda)$. \n",
    "\n",
    "\n",
    "* Remember, we have a **linear regression model**\n",
    "$$\\theta_0, \\theta_1 \\sim N(0,10)$$\n",
    "$$y|x,\\theta_0,\\theta_1 \\sim N(\\theta_0 x + \\theta_1, 0.1)$$\n",
    "\n",
    "* In our case, we have **two parameters** $\\theta=(\\theta_0,\\theta_1)$ and the **variational family** $q(\\theta|\\lambda)$ can be expressed as:\n",
    "\n",
    "$$q(\\theta|\\lambda) = q_0(\\theta_0|\\mu_0,\\sigma_0)q_1(\\theta_1|\\mu_1,\\sigma_1)$$\n",
    "\n",
    "$$q_i(\\theta_i|\\mu_i,\\sigma_i) = N(\\theta_i|\\mu_i,\\sigma_i)\\quad i=0,1$$\n",
    "\n",
    "* In this case, $q_i(\\theta_i|\\mu_i,\\sigma_i)$ is Gaussian. In consequence, \n",
    "$$ ϵ∼ N(0,I)$$\n",
    "$$ \\theta_i = \\mu_i + ϵ\\cdot\\sigma_i \\quad i=0,1$$\n",
    "\n",
    "\n",
    "* **For a given sample $\\epsilon$ from $\\phi(\\epsilon)=N(0,I)$** and $\\theta = f(ϵ,\\lambda) = \\mu + \\epsilon\\sigma$. Then, we have to:\n",
    "\n",
    "> 1. **Compute the gradient wrt $\\theta$** (the  code is given below): \n",
    "$$∇_\\theta \\ln \\frac{p(D,\\theta)}{q(\\theta|\\lambda)} = ∇_\\theta \\ln p(D,\\theta) - ∇_\\theta \\ln q(\\theta|\\lambda)$$ \n",
    "\n",
    "\n",
    "> > - **a)** We first compute $∇_\\theta \\ln p(\\theta)$:\n",
    "\n",
    "\n",
    "$$∇_{\\theta_j} \\ln p(\\theta_i) = -\\frac{\\theta_j}{\\sigma_p^2}\\quad i=0,1$$\n",
    "\n",
    "> > > where $\\sigma_p$ is the standard deviation of the prior $p(\\theta) = N(0,\\sigma_p)$. \n",
    "\n",
    "> > - **b)** Next step is to compute $∇_{\\theta} \\ln p(D|\\theta)$,\n",
    "\n",
    "$$∇_{\\theta_0} \\ln p(D|\\theta_0,\\theta_1) = \\sum_{i=1}^n \\frac{(y-\\theta_0 - \\theta_1 x_i)}{\\sigma_D^2}$$\n",
    "\n",
    "$$∇_{\\theta_1} \\ln p(D|\\theta_0,\\theta_1) = \\sum_{i=1}^n x_i\\frac{(y-\\theta_0 - \\theta_1 x_i)}{\\sigma_D^2}$$\n",
    "\n",
    "> > > where $\\sigma^2_D$ is the observation noise. \n",
    "\n",
    "> > - **c)** Finally, we compute $∇_{\\theta} \\ln q(\\theta|\\lambda)$:\n",
    "\n",
    "$$∇_{\\theta_i} \\ln q_i(\\theta_i|\\mu_i,\\sigma_i) = - \\frac{(\\theta_i - \\mu_i)}{\\sigma_i^2}\\quad i=0,1$$\n",
    "\n",
    "> 2. **Compute the gradient** $\\nabla_\\lambda f(ϵ,\\lambda)$. \n",
    "\n",
    "$$ \\nabla_{\\mu_i} f(ϵ,\\lambda) = 1 \\quad i=0,1 $$\n",
    "$$ \\nabla_{\\sigma_i} f(ϵ,\\lambda) = ϵ \\quad i=0,1 $$\n",
    "\n",
    "\n",
    "* **TASK FOR THIS EXERCISE:** Experiment with the number of Monte-Carlo samples $M$ per iteration, the learning-rate, and the number of iterations. Compare with the output of the Score Function Gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q99kjJjTTNvW"
   },
   "outputs": [],
   "source": [
    "def reparameterization_gradient(\n",
    "        q_distribution: Parameters,\n",
    "        prior: Parameters,\n",
    "        observation_noise: float,\n",
    "        data: Dict[str, np.ndarray],\n",
    ") -> GradientInfo:\n",
    "\n",
    "    \"\"\"\n",
    "    Gradient at theta defined using the re-parameterization-trick\n",
    "    :param q_distribution: Parameters for the q-distribution\n",
    "    :param prior: Parameters for the prior distribution\n",
    "    :param observation_noise: The observation noise in the regression model\n",
    "    :param data: A dictionary with the data. Has two keys: 'x' and 'y'\n",
    "    :return: The gradient info. \n",
    "    \"\"\"\n",
    "\n",
    "    # Sample a location -- now from Normal(0, I)\n",
    "    epsilon_sample = multivariate_normal(mean=np.array([0, 0]), cov=np.identity(n=2)).rvs(size=1)\n",
    "\n",
    "    # Translate back: theta = mu + epsilon * sigma, with product being element-wise\n",
    "    theta_sample = q_distribution.mean + np.multiply(epsilon_sample, q_distribution.std)\n",
    "\n",
    "    # Compute \\nabla_\\lambda log p(\\theta) = \\nabla_\\theta log p(\\theta) \\nabla_\\lambda f(\\epsilon,\\lambda)  \\:\n",
    "    #       For mu_j it is -theta_j/sigma_prior^2,\n",
    "    #       For sigma_j it is -theta_j/sigma_prior^2 * epsilon_j\n",
    "    gradient = Parameters(\n",
    "        mean=-1 * theta_sample / prior.std ** 2,\n",
    "        std=-1 * np.multiply(theta_sample / prior.std ** 2, epsilon_sample)\n",
    "    )\n",
    "\n",
    "    # Gradient of  \\nabla_\\lambda log p(Data|theta) = \\nabla_\\theta log p(Data|\\theta) \\nabla_\\lambda f(\\epsilon,\\lambda):\n",
    "    #   For mu_0 it is sum_i (y_i - theta_0 - theta_i * x_i) / sigma_data**2\n",
    "    #   For mu_1 it is sum_i (y_i - theta_0 - theta_i * x_i) * x_i / sigma_data**2\n",
    "    #   For sigma_0 it is sum_i (y_i - theta_0 - theta_1 * x_i) * epsilon_0 / sigma_data**2 + 1/sigma_0^2\n",
    "    #   For sigma_1 it is sum_i (y_i - theta_0 - theta_1 * x_i) * x_i * epsilon_1 / sigma_data**2 + 1/sigma_1^2\n",
    "\n",
    "    for i in range(data['x'].shape[0]):\n",
    "        x_i = data['x'][i]\n",
    "        y_i = data['y'][i]\n",
    "        error_i = (y_i - theta_sample[0] - theta_sample[1]*x_i) / (observation_noise ** 2)\n",
    "        delta_mean = np.array([error_i, error_i * x_i])\n",
    "        delta_std = np.multiply(delta_mean, epsilon_sample)\n",
    "\n",
    "        gradient.mean += delta_mean\n",
    "        gradient.std += delta_std\n",
    "\n",
    "    # Gradient of \\nabla_\\lambda log q(theta|lambda) = \\nabla_\\theta log q(theta|lambda) \\nabla_\\lambda f(\\epsilon,\\lambda)\n",
    "    gradient.mean -= -(theta_sample - q_distribution.mean)/q_distribution.std**2\n",
    "    gradient.std -=  -np.multiply((theta_sample - q_distribution.mean)/q_distribution.std**2, epsilon_sample)\n",
    "\n",
    "    # Put everything into the data-structure and leave\n",
    "    solution = GradientInfo(\n",
    "        theta=theta_sample,\n",
    "        score_function=None,\n",
    "        gradient=gradient,\n",
    "        aggregation=False)\n",
    "\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erx2e_L3muVQ"
   },
   "source": [
    "Since `reparameterization_gradient` works as is, just put it into the dict and run.\n",
    "Note that $M$ in the slides is called `no_samples_per_iter` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYJ4S8pKP7NE"
   },
   "outputs": [],
   "source": [
    "reparametrization_trick = {'name': 'Reparameterization-function', 'callable': reparameterization_gradient}\n",
    "\n",
    "compute_gradients(gradient_method=reparametrization_trick,\n",
    "                  no_samples_per_iter=5,\n",
    "                  learning_rate=0.0001, \n",
    "                  no_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk3NZstQkJCY"
   },
   "source": [
    "# Plot the evolution of the ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccNxZZHcQna3"
   },
   "outputs": [],
   "source": [
    "gradient_methods = [\n",
    "    {'name': 'Score-function', 'callable': score_function_gradient,},\n",
    "    {'name': 'Reparameterization-function', 'callable': reparameterization_gradient, },\n",
    "]\n",
    "\n",
    "# Check how many steps we have available\n",
    "data_points = np.min([len(store_elbo[gradient_method['name']]) for gradient_method in gradient_methods])\n",
    "\n",
    "# Make two plots for ELBO: One for almost the full range, the other for only the last 10%\n",
    "_, axes = plt.subplots(1, 2)\n",
    "\n",
    "# To make sure that the y-axis is not too much influenced by the first couple of steps, \n",
    "# it is advisable to start a bit into the sequence. Leave as is.\n",
    "start_point = 10\n",
    "\n",
    "# Go for it\n",
    "\n",
    "for gradient_method in gradient_methods:\n",
    "    axes[0].plot(range(start_point, 1 + data_points),\n",
    "                  store_elbo[gradient_method['name']][start_point - 1:data_points],\n",
    "                  label=gradient_method['name'])\n",
    "    axes[0].set_xlim(0, data_points)\n",
    "    axes[0].set_xlabel(\"Iteration\")\n",
    "    axes[0].set_ylabel(\"ELBO\")\n",
    "    axes[0].grid(True, which=\"both\")\n",
    "    \n",
    "    axes[1].yaxis.tick_right()\n",
    "    axes[1].plot(range(int(data_points * .9), 1 + data_points),\n",
    "                  store_elbo[gradient_method['name']][int(data_points * .9) - 1:data_points],\n",
    "                  label=gradient_method['name'])\n",
    "    axes[1].set_xlim(int(data_points * .9), data_points)\n",
    "    axes[1].set_xlabel(\"Iteration\")\n",
    "    axes[1].grid(True, which=\"both\")\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "plt.savefig(\"elbo.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "k147nsaRLE9n",
    "TrTSo0mVMNtp",
    "yW_APv_MMTPg"
   ],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
