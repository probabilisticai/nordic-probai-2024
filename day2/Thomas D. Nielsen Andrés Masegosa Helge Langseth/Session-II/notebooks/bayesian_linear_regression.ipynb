{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PGM-Lab/2023-probai-private/blob/main/Day2-AfterLunch/notebooks/bayesian_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXcRh2TQfyhp"
   },
   "source": [
    "## Setup\n",
    "Let's begin by installing and importing the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T11:52:00.267396Z",
     "start_time": "2023-05-11T11:52:00.054491Z"
    },
    "id": "EggRgZ1gfyhq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q numpy pandas seaborn matplotlib pyro-ppl torch pydot graphviz\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "from pyro.infer import Predictive\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Uniform, Delta, Gamma, Binomial\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro.optim as optim\n",
    "from pyro.contrib.autoguide import AutoNormal\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# for CI testing\n",
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtC9nacRfyhq"
   },
   "source": [
    "# 1. Dataset \n",
    "\n",
    "The following example is taken from \\[1\\].  We would like to explore the relationship between topographic heterogeneity of a nation as measured by the Terrain Ruggedness Index (variable *rugged* in the dataset) and its GDP per capita. In particular, it was noted by the authors in \\[1\\] that terrain ruggedness or bad geography is related to poorer economic performance outside of Africa, but rugged terrains have had a reverse effect on income for African nations. Let us look at the data \\[2\\] and investigate this relationship.  We will be focusing on three features from the dataset:\n",
    "  - `rugged`: quantifies the Terrain Ruggedness Index\n",
    "  - `cont_africa`: whether the given nation is in Africa\n",
    "  - `rgdppc_2000`: Real GDP per capita for the year 2000\n",
    " \n",
    "  \n",
    "We will take the logarithm for the response variable GDP as it tends to vary exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akcHul9xfyhr",
    "outputId": "406938bd-899e-45de-96b4-10fe76f393a1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/pyro-ppl/brmp/master/brmp/examples/rugged_data.csv\"\n",
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df.sample(frac=1)\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n",
    "df = df[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "\n",
    "# Divide the data into predictors and response and store the data in tensors organized in a dictionary with one\n",
    "# element for african and non-african nations, respectively.\n",
    "data = torch.tensor(df.values, dtype=torch.float)\n",
    "x_data = {'non-african': data[data[:, 0] == 0, 1].reshape(-1,1), 'african': data[data[:, 0] == 1, 1].reshape(-1,1)}\n",
    "y_data = {'non-african': data[data[:, 0] == 0, -1], 'african': data[data[:, 0] == 1, -1]}\n",
    "\n",
    "print(f\"Number of african countries: {x_data['african'].shape[0]}\")\n",
    "print(f\"Number of non-african countries: {x_data['non-african'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "EjOGg_Dafyhu",
    "outputId": "57a55f62-bbe5-4184-d1da-ed01d28c01ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display first 10 entries \n",
    "display(df[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vgTeHoTqChB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_figure(title='Scatter plot of data', x_data_: dict = None, y_data_: dict = None):\n",
    "    \"\"\"\n",
    "    Plot the data and return the figure axis for possible subsequent additional plotting.\n",
    "    :param title: Title of the plot\n",
    "    :param x_data_: dictionary with data for the prdictor variable\n",
    "    :param y_data_: dictionary with data for the response variable.\n",
    "    :return: Figure axis.\n",
    "    \"\"\"\n",
    "    if x_data_ is None and y_data_ is None:\n",
    "        x_data_ = x_data\n",
    "        y_data_ = y_data\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 6), sharey=True)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for idx, cont in enumerate(x_data):\n",
    "        ax[idx].scatter(x_data_[cont], y_data_[cont], c='black')\n",
    "        ax[idx].set(xlabel=\"Terrain Ruggedness Index\", ylabel=\"log GDP (2000)\",\n",
    "                    title=f\"{cont} nations\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLS8b0IVqChC",
    "outputId": "b1922059-3668-4866-c175-db1b1210b113",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's display the data\n",
    "prepare_figure('Scatter plot of data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFJeNXiUfyhx"
   },
   "source": [
    "# 1. Linear regression\n",
    "\n",
    "Linear regression is one of the most commonly used supervised learning tasks in machine learning. Suppose we are given a dataset $\\mathcal{D}$ of the form\n",
    "\n",
    "$$ \\mathcal{D}  = \\{ (X_i, y_i) \\} \\qquad \\text{for}\\qquad i=1,2,...,N$$\n",
    "\n",
    "The goal of linear regression is to fit a function to the data of the form:\n",
    "\n",
    "$$ y = w X + b + \\epsilon, $$\n",
    "\n",
    "where $w$ and $b$ are learnable parameters and $\\epsilon$ represents observation noise. Specifically $w$ is a matrix of weights and $b$ is a bias vector.\n",
    "\n",
    "Let's first implement linear regression in PyTorch and learn point estimates for the parameters $w$ and $b$.  Then we'll see how to incorporate uncertainty into our estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHG9QQYhfyhy",
    "tags": []
   },
   "source": [
    "## 2.2 Model specification\n",
    "We would like to predict log GDP per capita of a nation as a function of whether the nation is in Africa and its Terrain Ruggedness Index. As indicated by the data partitioning above, we will make one linear regression model for african and non-african nations, respectively. \n",
    "\n",
    "To sprcify our regression model, we will define a specific object encapsulating the model.  Our input `x_data` is a tensor of size $N \\times 1$ and our output `y_data` is a tensor of size $N \\times 1$.  The method `predict(self,x_data)` defines a linear transformation of the form $Xw + b$ where $w$ is the weight matrix and $b$ is the additive bias.\n",
    "\n",
    "The parameters of the model are defined using ``torch.nn.Parameter``, and will be learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBQBgFkPfyhz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionModel():\n",
    "    def __init__(self):\n",
    "        self.w = torch.nn.Parameter(torch.zeros(1, 1))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def params(self):\n",
    "        return {\"b\":self.b, \"w\": self.w}\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        return (self.b + torch.mm(self.w, torch.t(x_data))).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWllnzBWqChE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "regression_model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlGU_7YPfyhz"
   },
   "source": [
    "## 2.3 Training\n",
    "We will use the mean squared error (MSE) as our loss and Adam as our optimizer. We would like to optimize the parameters of the `regression_model` neural net above. We will use a somewhat large learning rate of `0.05` and run for 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_N6WPDJufyh0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def least_squares_solution(x_data, y_data, regression_model, verbose=True):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    optim = torch.optim.Adam(regression_model.params().values(), lr=0.05)\n",
    "    num_iterations = 1000\n",
    "\n",
    "    param = {}\n",
    "    for cont in x_data.keys():\n",
    "        param[cont] = {}\n",
    "        if verbose:\n",
    "            print(f\"Learning model for {cont} nations\")\n",
    "        for j in range(num_iterations):\n",
    "            # run the model forward on the data\n",
    "            y_pred = regression_model.predict(x_data[cont])\n",
    "            # calculate the mse loss\n",
    "            loss = loss_fn(y_pred, y_data[cont])\n",
    "            # initialize gradients to zero\n",
    "            optim.zero_grad()\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            # take a gradient step\n",
    "            optim.step()\n",
    "            if (j + 1) % 500 == 0 and verbose:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "\n",
    "        # Stored the learned parameters\n",
    "        param[cont]['w'] = np.r_[regression_model.params()['b'].detach().numpy().copy().flatten(),\n",
    "                      np.transpose(regression_model.params()['w'].detach().numpy()).copy().flatten()]\n",
    "    # Inspect learned parameters\n",
    "    if verbose:\n",
    "        print(\"Learned parameters:\")\n",
    "        for cont in param.keys():\n",
    "            print(f\"{cont}: weights = {param[cont]['w']}\")\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzJS4ZI4qChE",
    "outputId": "185c76ad-0882-4999-8a6f-97451bc918ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learn the model\n",
    "regression_model = RegressionModel()\n",
    "model_lr = least_squares_solution(x_data, y_data, regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyfAY0h-fyh0"
   },
   "source": [
    "## 2.4 Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmgazCZJfyh1"
   },
   "source": [
    "We now plot the regression line learned for african and non-afrian nations relating the rugeedness index with the GDP of the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "JWUs5dc1fyh1",
    "outputId": "f17e61f2-dfd1-44b5-e6fb-ca4c56969745",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = prepare_figure(title='Regression line')\n",
    "for idx, cont in enumerate(x_data.keys()):\n",
    "    ax[idx].plot(x_data[cont], model_lr[cont]['w'][0] + x_data[cont] * model_lr[cont]['w'][1], 'r') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJw5o9kdfyh1"
   },
   "source": [
    "## 2.5 The relationship between ruggedness and log GPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyWXJ1Tafyh2"
   },
   "source": [
    "Using this analysis, we can estimate the relationship between ruggedness and log GPD. As can be seen, this relationship is positive for African nations, but negative for Non African Nations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSB7vweCfyh2",
    "outputId": "bb18f50d-c866-4fa7-8ce1-fa2565771d27",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Slope for non-african nations: {model_lr['non-african']['w'][1]}\")\n",
    "print(f\"Slope for african nations: {model_lr['african']['w'][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yrEaqT5fyh3"
   },
   "source": [
    "# 3. Bayesian Linear Regression\n",
    "\n",
    "\n",
    "[Bayesian modeling](http://mlg.eng.cam.ac.uk/zoubin/papers/NatureReprint15.pdf) offers a systematic framework for reasoning about model uncertainty. Instead of just learning point estimates, we're going to learn a _distribution_ over parameters that are consistent with the observed data.\n",
    "\n",
    "In order to make our linear regression Bayesian, we need to put priors on the model parameters $w$, $b$ and $\\theta$. These are distributions that represent our prior belief about reasonable values for these parameters (before observing any data).\n",
    "\n",
    "A graphical representation would be as follows:\n",
    "\n",
    "<img src=\"https://github.com/PGM-Lab/probai-2021-pyro/raw/main/Day3/Figures/BayesianLinearRegressionPGM.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kltwl1J9fyh3"
   },
   "source": [
    "## 3.1 Model\n",
    "\n",
    "We now have all the ingredients needed to specify our model. Note the priors that we are using for the different latent variables in the model. The prior on the intercept parameter is very flat as we would like this to be learnt from the data. We are using a weakly regularizing prior on the regression coefficients to avoid overfitting to the data.\n",
    "\n",
    "We use the `obs` argument to the `pyro.sample` statement to condition on the observed data `y_data` with a learned observation noise parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_19buBJsfyh4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model(x_data_, y_data_):\n",
    "    # weight/slope prior\n",
    "    s = pyro.sample(\"s\", Normal(0, 1))\n",
    "\n",
    "    # bias/intercept prior\n",
    "    b = pyro.sample(\"b\", Normal(0., 1000.))\n",
    "\n",
    "    # Introduce a Gamma random variable called \"theta\" for modelling the precision\n",
    "    theta = pyro.sample(\"theta\", Gamma(1., 1.))\n",
    "\n",
    "    # Compute the predicion mean\n",
    "    mean = (b + x_data_ * s).squeeze(-1)\n",
    "    pyro.deterministic(\"predictive_mean\", mean)\n",
    "\n",
    "    with pyro.plate(\"plate_x\", len(x_data_)):\n",
    "        # Introduce a Normal distribution and condition on the observed data\n",
    "        pyro.sample(\"y\", Normal(loc=mean, scale=torch.sqrt(1. / theta)), obs=y_data_)\n",
    "\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nONjgsRIqChG",
    "outputId": "37def6ad-00d9-4c42-b557-090bbd31737f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyro.render_model(model, model_args=(x_data['african'], y_data['african']), render_distributions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcu6i1mYfyh6",
    "tags": []
   },
   "source": [
    "## 2.2 Guide\n",
    "\n",
    "In order to do inference we're going to need a guide, i.e. a variational family of distributions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06HOAET5qChG"
   },
   "source": [
    "### Autoguides\n",
    "We will use Pyro's [autoguide library](http://docs.pyro.ai/en/dev/contrib.autoguide.html) to automatically place Gaussians with diagonal covariance on all of the distributions in the model.  Under the hood, this defines a `guide` function with `Normal` distributions with learnable parameters corresponding to each `sample()` in the model.  Autoguide also supports learning MAP estimates with `AutoDelta` or composing guides with `AutoGuideList` (see the [docs](http://docs.pyro.ai/en/dev/contrib.autoguide.html) for more information). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMHO9NfFqChH"
   },
   "source": [
    "### Manually defined guides\n",
    "\n",
    "Alternatively, we can also define our own guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4UUrJnAqChH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def customized_guide(x_data, y_data):\n",
    "    q_b_loc = pyro.param(\"q_loc_b\", torch.tensor(0.0))\n",
    "    q_b_scale = pyro.param(\"q_scale_b\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    pyro.sample(\"b\", Normal(q_b_loc, q_b_scale))\n",
    "\n",
    "    q_s_loc = pyro.param(\"q_loc_s\", torch.tensor(0.0))\n",
    "    q_s_scale = pyro.param(\"q_scale_s\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    pyro.sample(\"s\", Normal(q_s_loc, q_s_scale))\n",
    "\n",
    "    q_theta_alpha = pyro.param(\"q_alpha_theta\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    q_theta_beta = pyro.param(\"q_beta_theta\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    pyro.sample(\"theta\", Gamma(q_theta_alpha, q_theta_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktwd6CCUfyh8"
   },
   "source": [
    "## 2.3 Inference\n",
    "\n",
    "To do inference we'll use stochastic variational inference (SVI) (for an introduction to SVI, see [SVI Part I](svi_part_i.ipynb)). Just like in the non-Bayesian linear regression, each iteration of our training loop will take a gradient step, with the difference that in this case, we'll use the ELBO objective instead of the MSE loss by constructing a `Trace_ELBO` object that we pass to `SVI`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py-1QUeyfyh9"
   },
   "source": [
    "Here `Adam` is a thin wrapper around `torch.optim.Adam` (see [here](svi_part_i.ipynb#Optimizers) for a discussion). To take an ELBO gradient step we simply call the step method of SVI. Notice that the data argument we pass to step will be passed to both model() and guide().  The complete training loop is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Wh1Tyqjfyh9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(x_data, y_data, model, guide_=None, num_iterations=1500, verbose=True):\n",
    "    optim = Adam({\"lr\": 0.1})\n",
    "    param = {}\n",
    "    for cont in x_data.keys():\n",
    "        if guide_ is None:\n",
    "            guide = AutoNormal(model)\n",
    "        else:\n",
    "            guide=guide_\n",
    "        svi = SVI(model, guide, optim, loss=Trace_ELBO(), num_samples=10)\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        for j in range(num_iterations):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step(x_data[cont], y_data[cont])\n",
    "            if j % 500 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j, loss / len(data)))\n",
    "        param[cont] = {'vi_parameters': pyro.get_param_store().get_state(),\n",
    "                       'guide': guide}\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV1XXdM0qChH",
    "outputId": "a172c12c-ee1a-4c30-92c2-8a808271760d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Insert you own guide if you like\n",
    "param = train(x_data, y_data, model, guide_=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfaPyhfTfyh9",
    "outputId": "62bcfd5b-3746-4629-e492-fda2ff4b1e6b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the parameters\n",
    "for cont in param.keys():\n",
    "    header = f\"\\nThe parameters for {cont} nations:\"\n",
    "    print(header)\n",
    "    print(f\"-\"*len(header))\n",
    "    for name, value in param[cont]['vi_parameters']['params'].items():\n",
    "        print(f\"{name:<25}: {value.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NndZ1K9qChI"
   },
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WO8lI1U2qChI"
   },
   "source": [
    "For model evaluation we rely on Pyro's Predictive class, which allows for easy sampling of the model parameters and subsequent estimations of the posterior predictive distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBkqziEXqChI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test = np.linspace(0, 6, 100)\n",
    "svi_samples = {}\n",
    "for cont in x_data.keys():\n",
    "    pyro.get_param_store().set_state(param[cont]['vi_parameters'])\n",
    "    predictive = pyro.infer.Predictive(model, guide=param[cont]['guide'], num_samples=1000)\n",
    "    #svi_samples[cont] = predictive(x_data[cont], None)\n",
    "    svi_samples[cont] = predictive(torch.tensor(x_test), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2X9dbJZJqChI"
   },
   "source": [
    "Disregarding our uncertainty estimates, we first plot the regression line defined by the predictive mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-rM_vtsqChI",
    "outputId": "8165c698-e31d-46dd-eaae-fbbb63ed6465",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = prepare_figure('Predictive mean')\n",
    "for idx, cont in enumerate(x_data.keys()):\n",
    "    y_predictive_mean = torch.squeeze(svi_samples[cont]['predictive_mean']).detach().numpy()\n",
    "    ax[idx].plot(x_test, np.mean(y_predictive_mean, axis=0), color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkT9oLJHqChI"
   },
   "source": [
    "If we lso take the uncertainty over the bias and slope into account, we can express the uncertain in our regression line. Below this is shown as the 95% CI around the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbIRuGjUqChI",
    "outputId": "b46749fc-effa-4692-ca9c-67e0e7c69713",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = prepare_figure('Predictive mean with 95% CI')\n",
    "for idx, cont in enumerate(x_data.keys()):\n",
    "    predictive_mean = torch.squeeze(svi_samples[cont]['predictive_mean']).detach().numpy()\n",
    "    predictive_std = np.std(predictive_mean, axis=0)\n",
    "    ax[idx].fill_between(x_test,\n",
    "                         np.mean(predictive_mean, axis=0) - 2 * predictive_std,\n",
    "                         np.mean(predictive_mean, axis=0) + 2 * predictive_std,\n",
    "                         color='b',\n",
    "                         alpha=0.1)\n",
    "    ax[idx].plot(x_test, np.mean(predictive_mean, axis=0), color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK1Ay_0PqChJ"
   },
   "source": [
    "If we also take the uncertainty in the response variable into account, i.e., sigma, we get the following 95% CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSI2oCpAqChJ",
    "outputId": "3c331672-737a-4725-c69d-3f9c14765b34",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = prepare_figure('Posterior predictive with 95% CI')\n",
    "for idx, cont in enumerate(x_data.keys()):\n",
    "    y_predictive_mean = torch.squeeze(svi_samples[cont]['y']).detach().numpy()\n",
    "    y_predictive_std = np.std(y_predictive_mean, axis=0)\n",
    "    ax[idx].fill_between(x_test,\n",
    "                         np.mean(y_predictive_mean, axis=0) - 2 * y_predictive_std,\n",
    "                         np.mean(y_predictive_mean, axis=0) + 2 * y_predictive_std,\n",
    "                         color='b',\n",
    "                         alpha=0.1)\n",
    "    ax[idx].plot(x_test, np.mean(y_predictive_mean, axis=0), color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znVVReeKqChJ"
   },
   "source": [
    "Lastly, we can revisit the question about the relationship between GDP and ruggedness by plotting the distribution of the slope for ruggedness for african and nn-african nations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrI35CzpqChJ",
    "outputId": "4c6bbe71-e9dc-4211-faaa-7d8d9764bbdd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(pd.DataFrame({'African': svi_samples['african']['s'].squeeze().tolist(), 'Non-african': svi_samples['non-african']['s'].squeeze().tolist()}))\n",
    "fig.suptitle(\"Density of Slope : log(GDP) vs. Terrain Ruggedness\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpo6kGPRfyiL"
   },
   "source": [
    "### References\n",
    "  1. McElreath, D., *Statistical Rethinking, Chapter 7*, 2016\n",
    "  2. Nunn, N. & Puga, D., *[Ruggedness: The blessing of bad geography in Africa\"](https://diegopuga.org/papers/rugged.pdf)*, Review of Economics and Statistics 94(1), Feb. 2012"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "include_colab_link": true,
   "name": "bayesian_linear_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
